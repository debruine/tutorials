[
["sim-data.html", "Chapter 2 Simulating Data 2.1 Independent samples 2.2 Paired samples 2.3 Intercept model 2.4 Functions", " Chapter 2 Simulating Data This tutorial details a few ways I simulate data. I'll be using some functions from my faux package to make it easier to generate sets of variables with specific correlations. library(tidyverse) library(faux) # devtools::install_github(&quot;debruine/faux&quot;) set.seed(8675309) # this makes sure your script uses the same set of random numbers each time you run the full script (never set this inside a function or loop) 2.1 Independent samples Let's start with a simple independent-samples design where the variables are from a normal distribution. Each subject produces one score (in conditions A or B). What we need to know about these scores is: How many subjects are in each condition? What are the score means? What are the score variances? I start simulation scripts by setting parameters for these values. A_sub_n &lt;- 50 B_sub_n &lt;- 50 A_mean &lt;- 10 B_mean &lt;- 12 A_sd &lt;- 2.5 B_sd &lt;- 2.5 We can the generate the scores using the rnorm() function. A_scores &lt;- rnorm(A_sub_n, A_mean, A_sd) B_scores &lt;- rnorm(B_sub_n, B_mean, B_sd) I always use the tidyverse for data wrangling&quot;, so I'll create a data table using the tibble() function. We need to know what condition each subject is in, so set the first A_sub_n values to &quot;A&quot; and the next B_sub_n values to &quot;B&quot;. Then set the score to the A_scores concatenated to the B_scores. dat &lt;- tibble( sub_condition = rep( c(&quot;A&quot;, &quot;B&quot;), c(A_sub_n, B_sub_n) ), score = c(A_scores, B_scores) ) Always examine your simulated data after you generate it to make sure it looks like you want. dat %&gt;% group_by(sub_condition) %&gt;% summarise(n = n() , mean = mean(score), sd = sd(score)) %&gt;% knitr::kable() sub_condition n mean sd A 50 10.25673 2.149247 B 50 12.00478 2.499963 Now you can analyse your simulated data. t.test(score~sub_condition, dat) ## ## Welch Two Sample t-test ## ## data: score by sub_condition ## t = -3.7492, df = 95.843, p-value = 0.0003034 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.6735571 -0.8225523 ## sample estimates: ## mean in group A mean in group B ## 10.25673 12.00478 2.2 Paired samples Now let's try a paired-samples design where the variables are from a normal distribution. Each subject produces two scores (in conditions A and B). What we need to know about these two scores is: How many subjects? What are the score means? What are the score variances? What is the correlation between the scores? sub_n &lt;- 100 A_mean &lt;- 10 B_mean &lt;- 12 A_sd &lt;- 2.5 B_sd &lt;- 2.5 AB_r &lt;- 0.5 You can then use rnorm_multi() to generate a data table with simulated values for correlated scores: dat &lt;- faux::rnorm_multi( n = sub_n, vars = 2, cors = AB_r, mu = c(A_mean, B_mean), sd = c(A_sd, B_sd), varnames = c(&quot;A&quot;, &quot;B&quot;) ) Now check your data; faux has a function check_sim_stats() that gives you the correlation table, means, and SDs for each numeric column in a data table. var A B mean sd A 1.00 0.48 9.90 2.46 B 0.48 1.00 11.88 2.53 Finally, you can analyse your simulated data. t.test(dat$A, dat$B, paired = TRUE) ## ## Paired t-test ## ## data: dat$A and dat$B ## t = -7.7821, df = 99, p-value = 7.01e-12 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.494424 -1.480841 ## sample estimates: ## mean of the differences ## -1.987633 2.3 Intercept model Now I'm going to show you a different way to simulate the same design. This might seem excessively complicated, but you will need this pattern when you start simulating data for mixed effects models. 2.3.1 Parameters Remember, we used the following parameters to set up our simulation above: sub_n &lt;- 100 A_mean &lt;- 10 B_mean &lt;- 12 A_sd &lt;- 2.5 B_sd &lt;- 2.5 AB_r &lt;- 0.5 From these, we can calculate the grand intercept (the overall mean regardless of condition), and the effect of condition (the mean of B minus A). grand_i &lt;- (A_mean + B_mean)/2 AB_effect &lt;- B_mean - A_mean We also need to think about variance a little differently. First, calculate the pooled variance as the mean of the variances for A and B (remember, variance is SD squared). pooled_var &lt;- (A_sd^2 + B_sd^2)/2 If the SDs for A and B are very different, this suggests a more complicated data generation model. For this tutorial we'll assume the score variance is similar for conditions A and B. The variance of the subject intercepts is r times this pooled variance and the error variance is what is left over. We take the square root (sqrt()) to set the subject intercept and error SDs for simulation later. sub_sd &lt;- sqrt(pooled_var * AB_r) error_sd &lt;- sqrt(pooled_var * (1-AB_r)) You can think about the subject intercept variance as how much subjects vary in the score in general, regardless of condition. If they vary a lot, in comparison to the random &quot;error&quot; variation, then scores in the two conditions will be highly correlated. If they don't vary much (or random variation from trial to trial is quite large), then scores won't be well correlated. 2.3.2 Subject intercepts Now we use these variables to create a data table for our subjects. Each subject gets an ID and a random intercept (sub_i). The intercept is simulated from a random normal distribution with a mean of 0 and an SD of sub_sd. This represents how much higher or lower than the average score each subject tends to be (regardless of condition). sub &lt;- tibble( sub_id = 1:sub_n, sub_i = rnorm(sub_n, 0, sub_sd) ) 2.3.3 Observations Next, set up a table where each row represents one observation. We'll use one of my favourite functions for simulation: expand.grid(). This creates every possible combination of the listed factors. Here, we're using it to create a row for each subject in each condition, since this is a fully within-subjects design. obs &lt;- expand.grid( sub_id = 1:sub_n, condition = c(&quot;A&quot;, &quot;B&quot;) ) 2.3.4 Calculate the score Next, we join the subject table so each row has the information about the subject's random intercept and then calculate the score. I've done it in a few steps below for clarity. The score is just the sum of: the overall mean (grand_i) the subject-specific intercept (sub_i) the effect of condition (-50% of AB_effect for condition A and +50% of AB_effect for condition B) the error term (simulated from a normal distribution with mean of 0 and SD of error_sd) dat &lt;- obs %&gt;% left_join(sub, by = &quot;sub_id&quot;) %&gt;% mutate( condition.e = recode(condition, &quot;A&quot; = -0.5, &quot;B&quot; = 0.5), effect = AB_effect * condition.e, error = rnorm(nrow(.), 0, error_sd), score = grand_i + sub_i + effect + error ) The variable condition.e &quot;effect codes&quot; condition, which we will use later in a mixed effect model. You can learn more about coding schemes here. You can use the following code to put the data table into a more familiar &quot;wide&quot; format. dat_wide &lt;- dat %&gt;% select(sub_id, condition, score) %&gt;% spread(condition, score) Then you can use the check_sim_stats function to check this looks correct (remove the subject ID to leave it out of the table, since it's numeric). var A B mean sd A 1.00 0.54 9.61 2.72 B 0.54 1.00 11.83 2.72 2.3.5 Analyses You can analyse the data with a paired-samples t-test from the wide format: t.test(dat_wide$A, dat_wide$B, paired = TRUE) ## ## Paired t-test ## ## data: dat_wide$A and dat_wide$B ## t = -8.4641, df = 99, p-value = 2.408e-13 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.738493 -1.698367 ## sample estimates: ## mean of the differences ## -2.21843 Or in the long format: t.test(score ~ condition, dat, paired = TRUE) ## ## Paired t-test ## ## data: score by condition ## t = -8.4641, df = 99, p-value = 2.408e-13 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.738493 -1.698367 ## sample estimates: ## mean of the differences ## -2.21843 You can analyse the data with ANOVA. afex::aov_4(score ~ (condition.e | sub_id), data = dat) ## Anova Table (Type 3 tests) ## ## Response: score ## Effect df MSE F ges p.value ## 1 condition.e 1, 99 3.43 71.64 *** .14 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 You can even analyse the data with a mixed effects model. lmem &lt;- lmerTest::lmer(score ~ condition.e + (1 | sub_id), data = dat) summary(lmem)$coefficients %&gt;% # nicer formatting with p-values as_tibble(rownames = &quot;Factor&quot;) %&gt;% mutate_if(~max(.) &lt; .001, ~as.character(signif(., 3))) %&gt;% knitr::kable(digits = 3) Factor Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 10.716 0.238 99 44.985 1.04e-67 condition.e 2.218 0.262 99 8.464 2.41e-13 2.4 Functions We can put everything together into a function where you specify the subject number, means, SDs and correlation, it translates this into the intercept specification, and returns a data table. sim_paired_data &lt;- function(n = 100, mu = c(0,0), sd = c(1,1), r = 0) { sub_n &lt;- n A_mean &lt;- mu[1] B_mean &lt;- mu[2] A_sd &lt;- sd[1] B_sd &lt;- sd[2] AB_r &lt;- r grand_i &lt;- (A_mean + B_mean)/2 AB_effect &lt;- B_mean - A_mean pooled_var &lt;- (A_sd^2 + B_sd^2)/2 sub_sd &lt;- sqrt(pooled_var * AB_r) error_sd &lt;- sqrt(pooled_var * (1-AB_r)) sub &lt;- tibble( sub_id = 1:sub_n, sub_i = rnorm(sub_n, 0, sub_sd) ) expand.grid( sub_id = 1:sub_n, condition = c(&quot;A&quot;, &quot;B&quot;) ) %&gt;% left_join(sub, by = &quot;sub_id&quot;) %&gt;% mutate( effect = case_when( condition == &quot;A&quot; ~ AB_effect * -0.5, condition == &quot;B&quot; ~ AB_effect * +0.5 ), error = rnorm(nrow(.), 0, error_sd), score = grand_i + sub_i + effect + error ) %&gt;% select(sub_id, condition, score) %&gt;% spread(condition, score) } sim_paired_data(100, c(10, 12), c(2.5, 2.5), .5) %&gt;% select(-sub_id) %&gt;% faux::check_sim_stats(usekable = TRUE) var A B mean sd A 1.00 0.48 9.98 2.32 B 0.48 1.00 11.97 2.62 sim_paired_data(10000, c(0, 0.5), c(1, 1), .25) %&gt;% select(-sub_id) %&gt;% faux::check_sim_stats(usekable = TRUE) var A B mean sd A 1.00 0.26 -0.02 1.00 B 0.26 1.00 0.51 0.99 It might be more suerful to create functions to translate back and forth from the distribution specification to the intercept specification. 2.4.1 Distribution to intercept specification dist2int &lt;- function(mu = 0, sd = 1, r = 0) { A_mean &lt;- mu[1] B_mean &lt;- ifelse(is.na(mu[2]), mu[1], mu[2]) A_sd &lt;- sd[1] B_sd &lt;- ifelse(is.na(sd[2]), sd[1], sd[2]) AB_r &lt;- r pooled_var &lt;- (A_sd^2 + B_sd^2)/2 list( grand_i = (A_mean + B_mean)/2, AB_effect = B_mean - A_mean, sub_sd = sqrt(pooled_var * AB_r), error_sd = sqrt(pooled_var * (1-AB_r)) ) } dist2int() grand_i 0 AB_effect 0 sub_sd 0 error_sd 1 dist2int(mu = c(100, 105), sd = c(10.5, 9.5), r = 0.5) grand_i 102.50 AB_effect 5.00 sub_sd 7.08 error_sd 7.08 2.4.2 Intercept to distribution specification int2dist &lt;- function(grand_i = 0, AB_effect = 0, sub_sd = 0, error_sd = 1) { pooled_var &lt;- sub_sd^2 + error_sd^2 list( A_mean = grand_i - 0.5 * AB_effect, B_mean = grand_i + 0.5 * AB_effect, A_sd = sqrt(pooled_var), B_sd = sqrt(pooled_var), AB_r = sub_sd^2 / pooled_var ) } int2dist() A_mean 0 B_mean 0 A_sd 1 B_sd 1 AB_r 0 int2dist(102.5, 5, 0.708, 0.708) A_mean 100.000 B_mean 105.000 A_sd 1.001 B_sd 1.001 AB_r 0.500 We can then use either sepcification to generate data with either technique. "]
]
